{"cells":[{"cell_type":"code","execution_count":49,"id":"81a07c66-a3d4-4fdd-9c3c-7b3a19b80d62","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":698,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1713202615065,"lastExecutedByKernel":"b94fe02a-d0f1-4be2-8796-10771d4d1f4c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nfrom pyspark.sql import SparkSession\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Initialize Spark session\nmy_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n\n# Importing sales data\nsales_data = my_spark.read.csv(\n    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n\n# Convert InvoiceDate to datetime \nsales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n\nsales_data.head()","outputsMetadata":{"0":{"height":59,"type":"stream"},"1":{"height":59,"type":"stream"},"2":{"height":38,"type":"stream"},"3":{"height":514,"type":"stream"}}},"outputs":[{"data":{"text/plain":["Row(InvoiceNo=536365, StockCode='85123A', Description='WHITE HANGING HEART T-LIGHT HOLDER', Quantity=6, UnitPrice=2.55, CustomerID=17850, Country='United Kingdom', InvoiceDate=datetime.date(2010, 1, 12), Year=2010, Month=1, Week=2, Day=12, DayOfWeek=1)"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# Import required libraries\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.regression import RandomForestRegressor\n","from pyspark.sql.functions import col, dayofmonth, month, year,  to_date, to_timestamp, weekofyear, dayofweek\n","from pyspark.ml.feature import StringIndexer\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# Initialize Spark session\n","my_spark = SparkSession.builder.appName(\"SalesForecast\").getOrCreate()\n","\n","# Importing sales data\n","sales_data = my_spark.read.csv(\n","    \"Online Retail.csv\", header=True, inferSchema=True, sep=\",\")\n","\n","# Convert InvoiceDate to datetime \n","sales_data = sales_data.withColumn(\"InvoiceDate\", to_date(\n","    to_timestamp(col(\"InvoiceDate\"), \"d/M/yyyy H:mm\")))\n","\n","sales_data.head()"]},{"cell_type":"code","execution_count":50,"id":"b5106e04-f9da-459f-a1cc-14e437fe001d","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":68,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1713202615133,"lastExecutedByKernel":"b94fe02a-d0f1-4be2-8796-10771d4d1f4c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"daily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\",\"UnitPrice\": \"avg\"})\ndaily_sales_data = daily_sales_data.withColumnRenamed(\"sum(Quantity)\", \"Quantity\")\n\n# Splitting the data into training and testing sets based on a specific date\ntrain_data = daily_sales_data.filter(col(\"InvoiceDate\") <= \"2011-09-25\")\ntest_data = daily_sales_data.filter(col(\"InvoiceDate\") > \"2011-09-25\")\n\n"},"outputs":[],"source":["daily_sales_data = sales_data.groupBy(\"Country\", \"StockCode\", \"InvoiceDate\", \"Year\", \"Month\", \"Day\", \"Week\", \"DayOfWeek\").agg({\"Quantity\": \"sum\",\"UnitPrice\": \"avg\"})\n","daily_sales_data = daily_sales_data.withColumnRenamed(\"sum(Quantity)\", \"Quantity\")\n","\n","# Splitting the data into training and testing sets based on a specific date\n","train_data = daily_sales_data.filter(col(\"InvoiceDate\") <= \"2011-09-25\")\n","test_data = daily_sales_data.filter(col(\"InvoiceDate\") > \"2011-09-25\")\n","\n"]},{"cell_type":"code","execution_count":51,"id":"16881cce-6c06-458a-a8df-257c600bb25b","metadata":{"executionCancelledAt":null,"executionTime":3249,"lastExecutedAt":1713202618382,"lastExecutedByKernel":"b94fe02a-d0f1-4be2-8796-10771d4d1f4c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# pandas DF\npd_daily_train_data = train_data.toPandas()","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# pandas DF\n","pd_daily_train_data = train_data.toPandas()"]},{"cell_type":"code","execution_count":52,"id":"fd82dc56-42c1-482c-90ff-055bc5816aa3","metadata":{"executionCancelledAt":null,"executionTime":10720,"lastExecutedAt":1713202629103,"lastExecutedByKernel":"b94fe02a-d0f1-4be2-8796-10771d4d1f4c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Regression Model\n#indexing\ncountry_index=StringIndexer(inputCol=\"Country\", outputCol=\"country_index\").setHandleInvalid(\"keep\")\nstock_index=StringIndexer(inputCol=\"StockCode\", outputCol=\"Stock_index\").setHandleInvalid(\"keep\")\n#Creating the vector \nfeature_cols = [\"country_index\", \"Stock_index\", \"Month\", \"Year\",\"DayOfWeek\", \"Day\", \"Week\"]\nassembler= VectorAssembler( inputCols=feature_cols , outputCol=\"features\" )\n#the regression model\nrf=RandomForestRegressor(featuresCol=\"features\", labelCol=\"Quantity\" , maxBins=4000)\n#pipeline\nsales_piped= Pipeline(stages=[country_index, stock_index, assembler, rf])\n#create the model\nmodel = sales_piped.fit(train_data)\n","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":38,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":38,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["24/04/15 17:37:04 WARN DAGScheduler: Broadcasting large task binary with size 1522.4 KiB\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["24/04/15 17:37:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#Regression Model\n","#indexing\n","country_index=StringIndexer(inputCol=\"Country\", outputCol=\"country_index\").setHandleInvalid(\"keep\")\n","stock_index=StringIndexer(inputCol=\"StockCode\", outputCol=\"Stock_index\").setHandleInvalid(\"keep\")\n","#Creating the vector \n","feature_cols = [\"country_index\", \"Stock_index\", \"Month\", \"Year\",\"DayOfWeek\", \"Day\", \"Week\"]\n","assembler= VectorAssembler( inputCols=feature_cols , outputCol=\"features\" )\n","#the regression model\n","rf=RandomForestRegressor(featuresCol=\"features\", labelCol=\"Quantity\" , maxBins=4000)\n","#pipeline\n","sales_piped= Pipeline(stages=[country_index, stock_index, assembler, rf])\n","#create the model\n","model = sales_piped.fit(train_data)\n"]},{"cell_type":"code","execution_count":53,"id":"4564d4b0-1f60-42e5-aaa2-1d4df78a69d4","metadata":{"executionCancelledAt":null,"executionTime":2706,"lastExecutedAt":1713202631809,"lastExecutedByKernel":"b94fe02a-d0f1-4be2-8796-10771d4d1f4c","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Evaluation\n\ntest_prediction= model.transform(test_data)\n\ntest_prediction= test_prediction.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n\nmae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"Quantity\", predictionCol=\"prediction\")\n\nmae=mae_evaluator.evaluate(test_prediction)\n\n\n# Getting the weekly sales of all countries\nweekly_test_predictions = test_prediction.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n\n# Finding the quantity sold on the 39 week. \npromotion_week = weekly_test_predictions.filter(col('Week')==39)\n\n# Storing prediction as quantity_sold_w30\nquantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])\n\n# Stop the Spark session\nmy_spark.stop()","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":143,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#Evaluation\n","\n","test_prediction= model.transform(test_data)\n","\n","test_prediction= test_prediction.withColumn(\"prediction\", col(\"prediction\").cast(\"double\"))\n","\n","mae_evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=\"Quantity\", predictionCol=\"prediction\")\n","\n","mae=mae_evaluator.evaluate(test_prediction)\n","\n","\n","# Getting the weekly sales of all countries\n","weekly_test_predictions = test_prediction.groupBy(\"Year\", \"Week\").agg({\"prediction\": \"sum\"})\n","\n","# Finding the quantity sold on the 39 week. \n","promotion_week = weekly_test_predictions.filter(col('Week')==39)\n","\n","# Storing prediction as quantity_sold_w30\n","quantity_sold_w39 = int(promotion_week.select(\"sum(prediction)\").collect()[0][0])\n","\n","# Stop the Spark session\n","my_spark.stop()"]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
